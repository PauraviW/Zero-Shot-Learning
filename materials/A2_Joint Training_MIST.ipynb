{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 Joint Training of Image and Semantic Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download dataset from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose(\n",
    "#     [\n",
    "#         transforms.ToPILImage(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "#     ]\n",
    "# )\n",
    "# trainset = datasets.MNIST(\"data/\", download=False, train=True, transform=transform)\n",
    "# valset = datasets.MNIST(\"data/\", download=False, train=False, transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "# valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the required network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, alpha, output_dim, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5)\n",
    "        self.lin = nn.Linear(20 * 20 * 32, output_dim)\n",
    "        self.encoder_hidden_layer = nn.Linear(7, 5)\n",
    "        self.encoder_hidden_layer_2 = nn.Linear(5, 5)\n",
    "\n",
    "    def forward_cnn(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.lin(x.view(-1, 20 * 20 * 32))\n",
    "        return x\n",
    "\n",
    "    def forward_sem(self, x):\n",
    "        x = torch.relu(self.encoder_hidden_layer(x))\n",
    "        x = torch.relu(self.encoder_hidden_layer_2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_cnn(input1)\n",
    "        output2 = self.forward_sem(input2)\n",
    "        return output1, output2\n",
    "\n",
    "    def save(self, path):\n",
    "        T.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(T.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2)\n",
    "            + (label)\n",
    "            * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    def __init__(self, data, transform, classes_subset, shuffle=True):\n",
    "        self.data = (\n",
    "            data[data.label.isin(classes_subset)].sample(frac=1).reset_index(drop=True)\n",
    "            if shuffle\n",
    "            else data\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.attributes = pd.read_csv(\"mnist_attributes.csv\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "        pixels = np.array(item[1:]).astype(np.uint8).reshape((28, 28))\n",
    "        # we need to make sure approx 50% of images are in the same class\n",
    "        label = -1\n",
    "        should_get_same_class = random.randint(0, 1)\n",
    "        attributes = None\n",
    "        if should_get_same_class:\n",
    "            label = 0\n",
    "            # keep looping till the same class image is found\n",
    "            attributes = (\n",
    "                self.attributes[self.attributes.Digit == int(item[0])]\n",
    "                .iloc[:, 1:]\n",
    "                .values\n",
    "            )\n",
    "        else:\n",
    "            label = 1\n",
    "            while True:\n",
    "                random_item = np.random.randint(low=1, high=10, size=1)\n",
    "                if random_item != item[0]:\n",
    "                    break\n",
    "            attributes = (\n",
    "                self.attributes[self.attributes.Digit == int(random_item)]\n",
    "                .iloc[:, 1:]\n",
    "                .values\n",
    "            )\n",
    "\n",
    "        if self.transform is not None:\n",
    "            pixels = self.transform(pixels)\n",
    "\n",
    "        return pixels, attributes, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "transformation = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ]\n",
    ")\n",
    "train_split = 0.7\n",
    "n_rows = len(train)\n",
    "len_training = int(n_rows * train_split)\n",
    "train_indexes = np.random.choice(len(train), size=len_training, replace=False)\n",
    "train_dataset = SiameseNetworkDataset(\n",
    "    train.iloc[train_indexes],\n",
    "    transform=transformation,\n",
    "    classes_subset=[0, 1, 2, 3, 4, 5, 6],\n",
    ")\n",
    "# validation_dataset = SiameseNetworkDataset(\n",
    "#     train.iloc[~train_indexes], transform=transformation, classes_subset=[7, 9, 8]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1, 1, 0]], dtype=int64), 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAOZUlEQVR4nO3de4xc9XnG8efB2MYYSGzMxXEsbgVSUhITrU0DSUVLQh1H1FgVBUtFRqCatBBBhZRatGmQKhXaNFArCRSnOHaihBSVWCEJbeNaqShK63ohjrG5xBRcMDg2YIIhEF/Wb//YIVpgz2+WmTMX+/1+pNXMnHfOOa9m99kzM78583NECMDB75BeNwCgOwg7kARhB5Ig7EAShB1I4tBu7myCJ8ZhmtzNXQKp/FK/0J7Y7dFqbYXd9lxJSyWNk/SPEXFz6f6HabLO9vnt7BJAwdpYU1lr+Wm87XGSvizpE5LOkLTQ9hmtbg9AZ7Xzmn2OpCci4smI2CPpW5Lm19MWgLq1E/YZkp4ZcXtrY9mb2F5se9D24F7tbmN3ANrRTthHexPgbZ+9jYhlETEQEQPjNbGN3QFoRzth3ypp5ojb75X0XHvtAOiUdsK+TtKptk+yPUHSpZLuractAHVreegtIvbZvkbSv2l46G15RGyqrTMAtWprnD0i7pN0X029AOggPi4LJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJdnbIZ3ffsknOK9aV/dEex/uLQEcX6igVzi/WhTY8X6+gejuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7AcBzz6zsnbb4tuK6547cX+xPs4vF+s3XDGlWD/l+uraoSfMLK47dOy7i/VY93CxjjdrK+y2t0h6RdKQpH0RMVBHUwDqV8eR/bcj4oUatgOgg3jNDiTRbthD0g9sP2h78Wh3sL3Y9qDtwb3a3ebuALSq3afx50bEc7aPlbTa9mMRcf/IO0TEMknLJOkoT4029wegRW0d2SPiucblDkmrJM2poykA9Ws57LYn2z7yjeuSLpC0sa7GANSrnafxx0laZfuN7XwzIv61lq7wZnOqx9El6bpv3l1ZazaO3sxQlNffdOkXi/XtF1e/TzO+yb7HD/9tVZq9+tpi/bQrBpvsIZeWwx4RT0r6YI29AOgght6AJAg7kARhB5Ig7EAShB1IglNcDwCb/2RCsf7xSa93qZO3O1TjivUZ4w7v2L4f+93bi/X3Lf/jylrGYTmO7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsfeCnd8wu1jd97MtNtpDz19hsjL80Dj/n2vLpsccv/VFLPfUzjuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kETOAdo+89WP3VmsT3Trv6Z5j/1esf43J99TrJ85odkXPpd9cO1llbVjlpXPdb/2i3cV6xcevqtYL43D7+vcafZ9iyM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBOHsX7Lj6nGJ9zsT/abKF1n9N8ZfTivXLP/SnxfqDS75UrH/gv6vH0SXphCu3VtaGfv5ycd3rv/uHxfqFl9xWrJd8+rLvFOurbjqm5W33q6ZHdtvLbe+wvXHEsqm2V9ve3Lic0tk2AbRrLE/jV0ia+5ZlSyStiYhTJa1p3AbQx5qGPSLul7TzLYvnS1rZuL5S0kU19wWgZq2+QXdcRGyTpMblsVV3tL3Y9qDtwb3a3eLuALSr4+/GR8SyiBiIiIHxmtjp3QGo0GrYt9ueLkmNyx31tQSgE1oN+72SFjWuL5JUHscA0HNNB3Bt3yXpPEnTbG+V9DlJN0u62/aVkp6WdHEnmzzQ/Xyg/F5FO+erS9Ka16tfHh360mvFdSe9eFix/qmtHy3WZ9xSPt+92Vh6yWl/saFY/+SsC4v175/+3cra7ElPFdddpYNvnL3pX1lELKwonV9zLwA6iI/LAkkQdiAJwg4kQdiBJAg7kASnuNZg3PtPL9b//XeWNtlCe99r/FefuaJ6y4+sLa475fnyENOz/zKhWPdL64v1dux/rTxs+LNdJ3Rs3wcjjuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7DV46uKji/UTD21vHH3BE/OK9SNWP1JZ299k20PPP99CR/3hXSuOLN9hdnXp9PHlR+aFqz5crE+747/K++5DHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2Q8AT+2cWqy/55WfdamT/jL5iV0trzvJ5fP0XzveLW+7X3FkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRNOw215ue4ftjSOW3Wj7WdvrGz/lb1cA0HNjObKvkDR3lOW3RsSsxs999bYFoG5Nwx4R90va2YVeAHRQO6/Zr7G9ofE0f0rVnWwvtj1oe3CvdrexOwDtaDXst0s6RdIsSdskfaHqjhGxLCIGImJgvCa2uDsA7Wop7BGxPSKGImK/pK9ImlNvWwDq1lLYbU8fcXOBpI1V9wXQH5qez277LknnSZpme6ukz0k6z/YsSSFpi6SrOthj31sw/4Fet5DS5svfXayPc/WxbCiafaP+wadp2CNi4SiL7+xALwA6iE/QAUkQdiAJwg4kQdiBJAg7kARfJV2DTx39oyb3aG/K5qwOObz8uJ101rPFeml4bcu+14rrnnDvy8V6FKv9iSM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBODt65pDDDivWH//8mcX65vfd3vK+L/jPTxfrv/bjH7e87X7FkR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcvQbzBsvfpP2Ts7/e1vaPmvTLYn3cqSdX1vY/XT7nO3Z3dkqu/R89q7K2+7MvFdfd/P7Wx9ElaZ+GKmtHrJvU1rYPRBzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtlrEOveVazvP7v8LeOHyMX6/Wf+c7mB/6gufXj9JcVVdz04rVh/zwN7i/WnF1WPZUvS0rPvqqzNnVT+7vZm1u/ZV6xf8ffXVdaOX9rsu/4PPk2P7LZn2v6h7Udtb7J9bWP5VNurbW9uXE7pfLsAWjWWp/H7JF0fEb8u6TclXW37DElLJK2JiFMlrWncBtCnmoY9IrZFxEON669IelTSDEnzJa1s3G2lpIs61SSA9r2jN+hsnyjpLElrJR0XEduk4X8Iko6tWGex7UHbg3vV2c9hA6g25rDbPkLSPZKui4hdY10vIpZFxEBEDIzXxFZ6BFCDMYXd9ngNB/0bEfHtxuLttqc36tMl7ehMiwDq4IjysJBta/g1+c6IuG7E8s9LejEibra9RNLUiPhMaVtHeWqc7fNraPvAsuWfPlCsb/zIV4v1ZkNzB6rSKaiStOrVUV8Z/spf37GwWJ9+S77htbWxRrti56h/MGMZZz9X0mWSHra9vrHsBkk3S7rb9pWSnpZ0cR3NAuiMpmGPiAekykNLvsM0cIDi47JAEoQdSIKwA0kQdiAJwg4kwSmuXXDiJRuK9e9vLp8ie+HhY/7AYte9HnuK9e/9Ynpl7aZ/KI+TH39reZx8uvKNo7eDIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJNH0fPY6ZT2fvV3PfPacYn3PUdW/w+W/X572ePPu44v1m763oFg/bcXOYn1o0+PFOupVOp+dIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4O3AQYZwdAGEHsiDsQBKEHUiCsANJEHYgCcIOJNE07LZn2v6h7Udtb7J9bWP5jbaftb2+8TOv8+0CaNVYJonYJ+n6iHjI9pGSHrS9ulG7NSL+rnPtAajLWOZn3yZpW+P6K7YflTSj040BqNc7es1u+0RJZ0la21h0je0NtpfbnlKxzmLbg7YH92p3W80CaN2Yw277CEn3SLouInZJul3SKZJmafjI/4XR1ouIZRExEBED4zWxhpYBtGJMYbc9XsNB/0ZEfFuSImJ7RAxFxH5JX5E0p3NtAmjXWN6Nt6Q7JT0aEbeMWD5yes4FkjbW3x6Auozl3fhzJV0m6WHb6xvLbpC00PYsSSFpi6SrOtIhgFqM5d34BySNdn7sffW3A6BT+AQdkARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgia5O2Wz7eUn/N2LRNEkvdK2Bd6Zfe+vXviR6a1WdvZ0QEceMVuhq2N+2c3swIgZ61kBBv/bWr31J9NaqbvXG03ggCcIOJNHrsC/r8f5L+rW3fu1LordWdaW3nr5mB9A9vT6yA+gSwg4k0ZOw255r+3HbT9he0oseqtjeYvvhxjTUgz3uZbntHbY3jlg21fZq25sbl6POsdej3vpiGu/CNOM9fex6Pf1511+z2x4n6aeSPi5pq6R1khZGxCNdbaSC7S2SBiKi5x/AsP1bkl6V9LWI+I3Gsr+VtDMibm78o5wSEX/WJ73dKOnVXk/j3ZitaPrIacYlXSTpcvXwsSv09QfqwuPWiyP7HElPRMSTEbFH0rckze9BH30vIu6XtPMti+dLWtm4vlLDfyxdV9FbX4iIbRHxUOP6K5LemGa8p49doa+u6EXYZ0h6ZsTtreqv+d5D0g9sP2h7ca+bGcVxEbFNGv7jkXRsj/t5q6bTeHfTW6YZ75vHrpXpz9vVi7CPNpVUP43/nRsRH5L0CUlXN56uYmzGNI13t4wyzXhfaHX683b1IuxbJc0ccfu9kp7rQR+jiojnGpc7JK1S/01Fvf2NGXQblzt63M+v9NM03qNNM64+eOx6Of15L8K+TtKptk+yPUHSpZLu7UEfb2N7cuONE9meLOkC9d9U1PdKWtS4vkjSd3rYy5v0yzTeVdOMq8ePXc+nP4+Irv9Imqfhd+T/V9Kf96KHir5OlvSTxs+mXvcm6S4NP63bq+FnRFdKOlrSGkmbG5dT+6i3r0t6WNIGDQdreo96+4iGXxpukLS+8TOv149doa+uPG58XBZIgk/QAUkQdiAJwg4kQdiBJAg7kARhB5Ig7EAS/w98gEPH4h6MNgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = train_dataset[0]\n",
    "plt.imshow(im[0].reshape(28, 28))\n",
    "im[1], im[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_iterations = 250\n",
    "n_epochs = 10\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(alpha=learning_rate, batch_size=batch_size, output_dim=5).to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (lin): Linear(in_features=12800, out_features=5, bias=True)\n",
       "  (encoder_hidden_layer): Linear(in_features=7, out_features=5, bias=True)\n",
       "  (encoder_hidden_layer_2): Linear(in_features=5, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-c00cbb2884f0>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(pixels.reshape(1, 1, 28, 28)).to(device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      " Current loss 1.5724196434020996\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.5754878520965576\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.7049034833908081\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.178051471710205\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.080912709236145\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.7763777375221252\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.8219127655029297\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.059652328491211\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.6771239042282104\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.676023542881012\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.6118512749671936\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.4768859148025513\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.34465649724006653\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.7316420078277588\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.10235624760389328\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.8211919069290161\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.9178779125213623\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.0427056550979614\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.7766579389572144\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.12823797762393951\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 5.183145549381152e-05\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.946251630783081\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.034889429807662964\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.036361575126648\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.18178127706050873\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.6905068755149841\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.1463615894317627\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 2.0538482666015625\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.5943230986595154\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.7914944887161255\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.5341055989265442\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.10902110487222672\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.5572270154953003\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.3649776875972748\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.24578627943992615\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.1480584442615509\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.4020368158817291\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.10935287922620773\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0038543292321264744\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.2580008506774902\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.08876640349626541\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.1275063753128052\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.08163253962993622\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.2262517213821411\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.021187130361795425\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.3423055410385132\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.07325447350740433\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.09831438213586807\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.03540319949388504\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.38597211241722107\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0017682429170235991\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.2801092565059662\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.1784505546092987\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.2305179089307785\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.21244576573371887\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.5057745575904846\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.2682499587535858\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.5031574368476868\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.10027407854795456\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.04221726953983307\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.0\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 2.2242870330810547\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.06900215893983841\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.012209349311888218\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.15217691659927368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn.train()\n",
    "for epoch in range(0, 2):\n",
    "    for i, (pixels, attributes, label) in enumerate(train_dataset):\n",
    "        pixels, attributes, label = (\n",
    "            torch.tensor(pixels.reshape(1, 1, 28, 28)).to(device),\n",
    "            torch.tensor(attributes).float().to(device),\n",
    "            label,\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = cnn.forward(pixels, attributes)\n",
    "        loss_contrastive = criterion(output1, output2, label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        if i % 500 == 0:\n",
    "            print(\n",
    "                \"Epoch number {}\\n Current loss {}\\n\".format(\n",
    "                    epoch, loss_contrastive.item()\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitsDataset(Dataset):\n",
    "    def __init__(self, data, transform, classes_subset, shuffle=True):\n",
    "        self.data = (\n",
    "            data[data.label.isin(classes_subset)].sample(frac=1).reset_index(drop=True)\n",
    "            if shuffle\n",
    "            else data\n",
    "        )\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "        pixels = np.array(item[1:]).astype(np.uint8).reshape((28, 28))\n",
    "        label = item[0]\n",
    "        if self.transform is not None:\n",
    "            pixels = self.transform(pixels)\n",
    "\n",
    "        return pixels, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = DigitsDataset(\n",
    "    train.iloc[~train_indexes], transform=transformation, classes_subset=[7, 9, 8]\n",
    ")\n",
    "attributes_df = pd.read_csv(\"data/mnist_attributes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = validation_dataset[10]\n",
    "plt.imshow(i[0].reshape(28, 28))\n",
    "i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{7: tensor([0.0000, 0.0000, 1.1880, 0.9647, 0.1244], device='cuda:0'),\n",
       " 8: tensor([0.8560, 0.0000, 2.1143, 0.3307, 2.1965], device='cuda:0'),\n",
       " 9: tensor([0.0000, 0.0000, 2.3853, 1.5444, 1.8408], device='cuda:0')}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.eval()\n",
    "attrib = {}\n",
    "with torch.no_grad():\n",
    "    for r in attributes_df.to_numpy()[7 : 9 + 1]:\n",
    "        output1 = cnn.forward_sem(torch.tensor(r[1:]).float().to(device))\n",
    "        attrib[r[0]] = output1\n",
    "attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-97be43735b71>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixels = torch.tensor(pixels.reshape(1, 1, 28, 28)).to(device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.022547914317926"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.eval()\n",
    "with torch.no_grad():\n",
    "    n_correct_attributes = 0\n",
    "    pred_label = None\n",
    "    for i, (pixels, label) in enumerate(validation_dataset):\n",
    "        pixels = torch.tensor(pixels.reshape(1, 1, 28, 28)).to(device)\n",
    "        output1 = cnn.forward_cnn(pixels)\n",
    "        dist = sys.maxsize\n",
    "        for k, v in attrib.items():\n",
    "            z = F.pairwise_distance(output1, v).item()\n",
    "            if z < dist:\n",
    "                pred_label = k\n",
    "                dist = z\n",
    "        if pred_label == label:\n",
    "            n_correct_attributes += 1\n",
    "n_correct_attributes * 100.0 / len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dict = {\"optimizer\": optimizer.state_dict(), \"model\": cnn.state_dict()}\n",
    "torch.save(checkpoint_dict, \"A2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.load_state_dict(torch.load(\"A2\")[\"model\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
